<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="算法," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="1. 偏差-方差均衡​    我们学习线性回归时，曾讨论了选取简单的模型“\(y=\theta _{0}+\theta _{1}x\)”还是一个复杂的模型“\(y=\theta _{0}+\theta _{1}x +\cdots +\theta _{5}x^{5}\)”进行建模。我们看下面这个例子，​    选取5阶多项式得到的模型（最右边图示）并不是好模型。即使5阶多项式在预测训练集样本时可以取">
<meta name="keywords" content="算法">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习系列(6)——学习理论">
<meta property="og:url" content="http://yoursite.com/2017/09/25/0925-ml-06/index.html">
<meta property="og:site_name" content="Lizemin&#39;s Blog">
<meta property="og:description" content="1. 偏差-方差均衡​    我们学习线性回归时，曾讨论了选取简单的模型“\(y=\theta _{0}+\theta _{1}x\)”还是一个复杂的模型“\(y=\theta _{0}+\theta _{1}x +\cdots +\theta _{5}x^{5}\)”进行建模。我们看下面这个例子，​    选取5阶多项式得到的模型（最右边图示）并不是好模型。即使5阶多项式在预测训练集样本时可以取">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/image/20170925/01.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/02.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/03.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/04.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/05.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/06.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/07.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/08.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/09.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/10.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/11.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/12.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/13.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/14.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/15.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/16.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/17.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/18.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/19.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/20.png">
<meta property="og:image" content="http://yoursite.com/image/20170925/21.png">
<meta property="og:updated_time" content="2017-10-03T01:43:54.553Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习系列(6)——学习理论">
<meta name="twitter:description" content="1. 偏差-方差均衡​    我们学习线性回归时，曾讨论了选取简单的模型“\(y=\theta _{0}+\theta _{1}x\)”还是一个复杂的模型“\(y=\theta _{0}+\theta _{1}x +\cdots +\theta _{5}x^{5}\)”进行建模。我们看下面这个例子，​    选取5阶多项式得到的模型（最右边图示）并不是好模型。即使5阶多项式在预测训练集样本时可以取">
<meta name="twitter:image" content="http://yoursite.com/image/20170925/01.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/25/0925-ml-06/"/>





  <title>机器学习系列(6)——学习理论 | Lizemin's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lizemin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/25/0925-ml-06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lizemin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lizemin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习系列(6)——学习理论</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-25T07:44:25+08:00">
                2017-09-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/09/25/0925-ml-06/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/09/25/0925-ml-06/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/09/25/0925-ml-06/" class="leancloud_visitors" data-flag-title="机器学习系列(6)——学习理论">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-偏差-方差均衡"><a href="#1-偏差-方差均衡" class="headerlink" title="1. 偏差-方差均衡"></a>1. 偏差-方差均衡</h2><p>​    我们学习线性回归时，曾讨论了选取简单的模型“\(y=\theta _{0}+\theta _{1}x\)”还是一个复杂的模型“\(y=\theta _{0}+\theta _{1}x +\cdots +\theta _{5}x^{5}\)”进行建模。我们看下面这个例子，<br><img src="/image/20170925/01.png" alt="01"><br>​    选取5阶多项式得到的模型（最右边图示）并不是好模型。即使5阶多项式在预测训练集样本时可以取得很好的效果，但是在训练集以外的样本上的预测往往是不准确的。换句话说，从训练集学习到的参数在测试集上表现并不好。一个假设函数的<strong>泛化误差（generalization error）</strong>为对非训练集样本预测的误差。<br><a id="more"></a><br>​    最右边图示与最左边图示的模型都有很大的泛化误差，然而两个模型存在的问题是不一样的。如果y和x之间的关系是非线性的，那么即使我们用超大的训练集训练一个线性模型，这个线性模型依然很难准确把握数据的结构。不正式地，我们定义<strong>偏差（bias）</strong>表示即使使用很大数据集训练，最后预测的也存在的泛化误差。因此，在上面的问题中，线性模型就存在很大的偏差，也可以说对数据是<strong>欠拟合</strong>的。</p>
<p>​    除了偏差，在泛化误差里面还有另外一种成分，对于拟合过程中存在的<strong>方差</strong>。在最右图中，使用5阶多项式恰好可以比较好地拟合比较小的有限的训练集里的数据，但会存在一个比较大的风险，因为这个模型并不能反应在更广泛数据集中x与y的关系。换句话说，这个模型可能拟合了比平均值高一点的房屋价格的数据，但对低于平均值的房屋价格数据的拟合效果却是未知的。在训练集中“伪造”得到的模型会产生更大的泛化误差。这种情况，我们说这个模型有大的方差。</p>
<p>​    经常，在偏差和误差取得一种均衡。如果我们的模型太简单，参数较少，那么就可能会有大的偏差（小的方差）;如果模型太复杂，参数较多，那就可能会有比较大的方差（小的偏差）。在上面的例子中，用二次函数进行拟合的效果会更好。</p>
<h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h2><p>​    接下来，我们开始接触<strong>学习理论（learning theory）</strong>。主要解决以下几个问题：一是如何均衡刚刚讨论的偏差与方差？这也会引导我们思考模型的选择方法，自然也会告诉我们选取几阶多项式拟合模型；二是在机器学习中，我们比较关注泛化误差（非训练集样本的预测误差），但是大多学习算法拟合的是训练集。为什么在训练集上表现很好地模型可以告诉我们泛化误差的一些信息呢？尤其是，我们是否可以把在训练集上的误差和泛化误差联系起来？三是，是否存在一定条件下，我们可以确切证明学习算法的效果很好。</p>
<p>​    首先我们介绍两种简单但是有用的引理。</p>
<p><strong>引理1</strong>(联合界)。\(A _{1},A _{2},…,A _{k}\)为k个不同事件（可能不是独立的），那么<br><img src="/image/20170925/02.png" alt="02"><br>​    在概率论中。联合界被公认问一种公理（我们不去证明）。由此可知，k个事件中任意一个发生的概率小于k个事件分别发生的概率之和。</p>
<p><strong>引理2</strong>（Hoeffding 不等式）。\(Z _{1},Z _{2},…,Z _{m}\)表示m个由伯努利分布得到的独立同分布随机变量。\(P(Z _{i}=1)=\phi\)，\(P(Z _{i}=0)=1-\phi\)。\(\hat{\phi}=(1/m)\sum _{i=1}^{m}Z _{i}\)表示这些随机变量的平均值，对任意一个确定的变量\(\gamma &gt;0\)。有<br><img src="/image/20170925/03.png" alt="03"><br>​    该引理（在学习理论中也被称为<strong>切诺夫边界</strong>）说明如果我们将\(\hat{\phi}\)作为\(\phi\)的估计，若m足够大，那么概率值与真实值之间的误差是比较小的。换句话说，我们投掷一枚硬币，正面朝上的概率为φ，那么如果我们投掷m次，并且计算正面朝上次数与m的比值，最终可以得到比较可信的φ的取值（如果m足够大）。</p>
<p>​    利用这两个引理，我们可以证明在学习理论中的更深层次和更重要的结果。</p>
<p>​    为了简化表述，我们只关注二分类问题，样本标签为\(y\in \left \{ 0,1 \right \}\)。讨论的结果都可以延伸至回归问题和多分类问题等。</p>
<p>​    假设我们给定大小为m的训练集\(S=\left \{ (x ^{(i)},y ^{(i)});i=1,…,m \right \}\)，训练样本\(\left ( x ^{(i)},y ^{(i)} \right )\)来源于概率为D的独立同分布。对于假设h，我们定义<strong>训练误差（training error）</strong>（在学习理论中也被称为<strong>经验风险</strong>和<strong>经验误差</strong>）为，<br><img src="/image/20170925/04.png" alt="04"><br>​    这表示在假设函数h误分类的概率。当我们想明确表示在训练集S上经验误差时，我们可以写成\(\hat{\varepsilon } _{S}(h)\)。我们也可以将泛化误差定义为<br><img src="/image/20170925/05.png" alt="05"><br>​    该式表示，如果我们从分布D中取得一个样本(x, y)，h对其误分类的概率。</p>
<p>​    注意到我们之前已经假设训练数据来源于我们评估假设函数的分布D（在泛化误差的定义中的样本也是来自分布D）。</p>
<blockquote>
<p>这里定义了<strong>训练误差</strong>和<strong>泛化误差</strong>，它们的区别在于：<br><strong>训练误差</strong>来源于训练集，是可以通过训练集样本的预测直接获取的。<br><strong>泛化误差</strong>来源于非训练集，可以表征预测的效果，但这是未知的。因为我们并不知道我们训练的模型在预测新样本的时候的表现情况。<br>所以接下来会做的一件事就是，如何根据训练误差来推算泛化误差，以确定模型的效果。</p>
</blockquote>
<p>​    考虑到在线性分类中，\(h _{\theta}(x)=1\left \{ \theta ^{T}x\geq 0 \right \}\)。如何找到合适的参数θ呢？我们的办法是尝试最小化训练误差，并且选取<br><img src="/image/20170925/06.png" alt="06"><br>​    我们将这个过程称为<strong>经验风险最小化（empirical risk minimization, ERM）</strong>，并且通过学习理论得到的假设函数为\(\hat{h}=h _{\hat{\theta}}\)。ERM是最“基本”的学习算法，并且我们会在本节内容中重点讨论。值得注意的是，对于ERM来说，因为它是非凸的，故而一般的算法无法优化它。但是逻辑斯特回归算法和SVM都是这种方法的凸性近似。</p>
<p>​    在我们对学习理论的学习中，应该不考虑特别的参数化的假设以及我们是否使用的是线性分类器。我们定义了<strong>假设集H</strong>作为所有要考虑的分类器的集合。对于线性分类，\(H=\left \{ h _{\theta}:h _{\theta}(x)=1\left \{ \theta ^{T}x\geqslant 0 \right \},\theta\in \mathbb{R} ^{n+1} \right \}\)表示所有在输入空间上的分类器的集合，分类边界的是线性的。更广泛的，如果我们研究神经网络，那么我们可以使H表示神经网络所等价的分类器的集合。</p>
<p>​    经验风险最小化可以被看做在函数集H上的经验误差的最小化，在学习理论中将假设函数表示为：<br><img src="/image/20170925/07.png" alt="07"></p>
<blockquote>
<p>可以看到\(\hat{h}\)是通过最小化训练误差得到的，但不一定对于非训练集样本是最优的。</p>
</blockquote>
<h2 id="3-有限H的情况"><a href="#3-有限H的情况" class="headerlink" title="3. 有限H的情况"></a>3. 有限H的情况</h2><p>​    我们先考虑包含有限的假设集的情况，比如包含k个假设的假设集\(H=\left \{ h _{1},…, h _{k} \right \}\)。因此，H表示包含将输入空间X映射到{0, 1}的k个函数映射，并且经验风险最小化就是在k个函数中选取训练误差最小的函数作为\(\hat{h}\)。</p>
<p>​    我们想要得到\(\hat{h}\)的泛化误差。要达到这一目的有两步操作：1，说明对所有假设函数h都满足的\(\hat{\varepsilon }(h)\)与\(\varepsilon(h)\)之间的关系；2，说明这个关系暗含了\(\hat{h}\)的泛化误差的上界。</p>
<blockquote>
<p>即说明从经验误差可以得到泛化误差，然后从训练集中的最优预测函数可以得到泛化误差的上界，即可评估预测函数在非训练集上的表现。</p>
</blockquote>
<p>​    选取任何一个确定的\(h _{i}\in H\)。考虑一个伯努利变量Z的分布定义如下。选取一个样本\(\left ( x, y \right )\sim D\)，然后，设\(Z=1\left \{ h _{i}(x)\neq y \right \}\)。就是说，我们尝试得到一个样本，然后用Z表示假设\(h _{i}\)对该样本点的分类是否正确。相似地，我们定义\(Z _{j}=1\left \{ h _{i}(x ^{(j)})\neq y ^{(j)} \right \}\)。由于我们的训练集都是来源于独立同分布D，所以\(Z\)和\(Z _{j}\)有一样的分布。</p>
<p>​    我们将在随机获取的样本上的误分类概率\(\varepsilon(h)\)看成Z的期望。更多地，训练误差可以写成<br><img src="/image/20170925/08.png" alt="08"><br>​    因此，\(\hat{\varepsilon }(h _{i})\)确实是m个随机变量\(Z _{j}\)的平均值，\(Z _{j}\)来源于均值为\(\varepsilon(h _{i})\)的伯努利分布。因此，我们可以应用Hoeffding不等式，得到<br><img src="/image/20170925/09.png" alt="09"></p>
<blockquote>
<p>这里有必要解释一下，\(\varepsilon(h)\)是泛化误差，针对的是所有满足分布D的样本；<br>\(\hat{\varepsilon }(h _{i})\)是经验误差，来源于m个训练集样本。</p>
</blockquote>
<p>​    这说明，对于一个特定的\(h _{i}\)，假设m足够大时，训练误差很大概率接近于泛化误差。但是这一规律不能只对特定的\(h _{i}\)成立。我们要证明对所有\(h _{i}\in H\)满足这一规律。为了说明，我们设\(A _{i}\)表示事件\(\left | \varepsilon (h _{i})-\hat{\varepsilon}(h _{i})  \right |&gt;\gamma\)。因此，使用联合界引理，有<br><img src="/image/20170925/10.png" alt="10"><br>​    用1减去两边， 可以得到<br><img src="/image/20170925/11.png" alt="11"><br>​    因此，至少有\(1-2kexp(-2\gamma ^{2}m)\)的概率，我们可以对所有的\(h _{i}\in H\)，\(\varepsilon(h)\)都是\(\hat{\varepsilon }(h)\)为中心γ为半径的区域内的一点。这被称为<em>一致性收敛</em>，因为这是一个是所有\(h _{i}\in H\)都成立的一个边界。</p>
<p>​    在上面的讨论中，我们所做的就是给一个特殊的m和γ，给定一个概率上的边界，对一些\(h\in H\)，\(\left | \varepsilon (h)-\hat{\varepsilon}(h) \right |&gt; \gamma\)。在这里有三个变量：m，γ以及误差的概率；我们可以将其中的一个变量用另外两个变量表示。</p>
<p>​    例如，我们可以提出这样一个问题，给定γ和一些δ&gt;0，m要多大才能保证上述推导的概率大于1-δ，并且训练误差在γ范围内呢？设\(\delta =2kexp(-2\gamma ^{2}m)\)并且求解m，可以得到<br><img src="/image/20170925/12.png" alt="12"><br>​    那么对所有\(h\in H\)，满足\(\left | \varepsilon (h)-\hat{\varepsilon}(h) \right |\leq  \gamma\)的概率大于等于1-δ。（等价地，这说明对一些\(h \in H\)，\(\left | \varepsilon (h)-\hat{\varepsilon}(h) \right |&gt; \gamma\)的概率小于δ）这个不等式告诉我们在训练集中需要包含样本的数据。一个特定的方法或者算法取得一定地训练效果时所需要的训练集大小m也被称为<strong>样本复杂度，sample complexity</strong>。</p>
<p>​    在后面的讨论中，我们会提到，在训练集样本满足的不等式中，假设集H的大小k的对数也是很重要的。</p>
<p>​    同样地，我们可以固定m和δ不变，在之前的式子中求解γ，得到对所有\(h\in H\)，<br><img src="/image/20170925/13.png" alt="13"><br>​    现在，我们假设一致性收敛为，\(\left | \varepsilon (h)-\hat{\varepsilon}(h) \right |\leq  \gamma\)对所有\(h \in H\)成立。我们如何证明在所有的学习算法中我们要选择\(\hat{h}=argmin _{h \in H} \hat{\varepsilon}(h)\)？</p>
<blockquote>
<p>就是说，我们已经确定了对于假设集中的任意的一个h，它的训练误差和泛化误差都满足一定的关系。<br>接下来，我们要确定为什么通过训练集得到的最优的\(\hat {h}\)，与H中其他的可能最优的假设函数\(h ^{\star}\)相比，效果也是不错的。</p>
</blockquote>
<p>​    定义\(h ^{\star}=argmin _{h \in H}\varepsilon(h)\)为假设集H中最优的假设。注意\(h ^{\star}\)来源于我们给定的H，因此H中的其它假设可以与\(h ^{\star}\)做比较，有：<br><img src="/image/20170925/14.png" alt="14"><br>​    第一行根据之前的一致性收敛得到，第二行因为\(\hat{h}\)对应于极小值\(\hat{\varepsilon}(h)\)，因此\(\hat{\varepsilon}(\hat{h})\leq  \hat{\varepsilon}(h)\)对所有h成立，也对\(\hat{\varepsilon}(\hat{h})\leq  \hat{\varepsilon}(h ^{\star})\)成立。第三行再一次用到了一致性收敛假设，说明\(\hat{\varepsilon}(h ^{\star})\leq \varepsilon (h ^{\star})+\gamma\)。因此说明：<strong>如果一致性收敛的条件满足，那么\(\hat{h}\)的泛化误差小于H中最可能假设\(h ^{\star}\)的泛化误差再加上2γ</strong>。这样就得到了，在训练集上找到的最优的\(\hat {h}\)与H上可能存在的最优的\(h ^{\star}\)相比，效果也是不错的。</p>
<p>​    将所有的结论合并成一个定理可以得到。</p>
<p>​    <strong>定理。</strong>设\(\left | H \right |=k\)，取任意m，δ。那么至少有1-δ的概率满足，<br><img src="/image/20170925/15.png" alt="15"><br>​    这也可以用来量化我们之前所说的在模型选择时要考虑的误差方差均衡问题。假设我们有一个假设集H，并在考虑一个更大的假设集H’，\(H’\supseteq H\)。如果我们选择H’，那么第一项\(min _{h}\varepsilon(h)\)只会减少。因此，对于更大的假设集，我们的误差会减小。然而，如果k增加，那么第二项也会增加。在更大的假设集中方差也会相应增加。</p>
<p>​    保持γ和δ不变并且解出m，可以得到样本复杂度不等式：</p>
<p><strong>推论。</strong> 设\(\left | H \right |=k\)，取任意m，δ。那么要使\(\varepsilon (\hat{h})\leq min _{h}\varepsilon(h)+2\gamma\)成立的概率至少为1-δ，需满足<br><img src="/image/20170925/16.png" alt="16"></p>
<h2 id="4-无限大小的H的情况"><a href="#4-无限大小的H的情况" class="headerlink" title="4. 无限大小的H的情况"></a>4. 无限大小的H的情况</h2><p>​    我们已经在有限大小H中证明了很多有用的理论。对于有些假设集，包含参数化为无穷个实数的模型（线性分类）。我们如何证明相似的结果呢？</p>
<p>​    我们先看一些“不正确”的论证。尽管更普遍的论证存在，但是但是对于“不正确”的论断的讨论也是有用的。</p>
<p>​    假设我们有一个含d个实数参数的假设H。由于我们要在计算机中表示实数，同时一个双精度浮点型数据需要64位表示。因此，我们的假设集由最多有\(k=2 ^{64d}\)种不同假设。在上一部分的推论中，我们得到，为了保证最少有1-δ的概率使\(\varepsilon (\hat{h})\leq \varepsilon (h ^{\star})+2 \gamma\)成立，m需满足\(m\geq O(\frac{1}{\gamma ^{2}}log\frac{2 ^{64d}}{\delta})=O(\frac{d}{\gamma ^{2}}log\frac{1}{\delta})=O _{\gamma, \delta}(d)\)。因此，训练集样本数与模型的参数个数成正比。</p>
<p>​    在这里要注意的是我们证明的结果适用于使用了经验风险最小化的算法。因此，最后的结论在一些判别学习算法中并不是完全成立。为一些非ERM的学习算法中提供理论保证依然是研究的重点领域。</p>
<p>​    我们之前论证的另一部分在H参数化时也不满足。我们将一种线性分类器写成\(h _{\theta }(x)=1\left \{ \theta _{0}+\theta _{1}x _{1}+\cdots +\theta _{n}x _{n} \geqslant 0  \right \}\)，有n+1个参数\(\theta _{0},…,\theta _{n}\)。但是这个分类器也可以写成\(h _{u,v}(x)=1\left \{ (\mu _{0} ^{2}-\upsilon _{0} ^{2})+(\mu _{1} ^{2}-\upsilon _{1} ^{2})x _{1}+\cdots+(\mu _{n} ^{2}-\upsilon _{n} ^{2})x _{n} \geq 0 \right \}\)，有2n+1个单数μ，v。但是，两个式子定义地都是同一个假设H。</p>
<p>​    为了得到更加满意的论证，我们再定义一些东西。</p>
<p>​    给定一个集合\(S = \left \{ x ^{(i)},…,x ^{(d)} \right \}\)（与训练集没有关系）中的\(x ^{(i)} \in X\)，如果H可以对S中的样本的任意标签情况都实现分类，我们说就说H<strong>打散（shatters）</strong>S。意思是，对于任意一个标签集合\(\left \{ y ^{(i)},…,y ^{(d)} \right \}\)，存在一些\(h \in H\)使\( h(x ^{(i)}) = y ^{(i)}\)对所有的i = 1,…,d都成立。</p>
<p>​    给定一个假设集H，我们可以定义他的<strong>Vapnik-Chervonenkis维度</strong>，一般写为VC(H)，表示可以被H打散的最大集合的大小。（如果H可以打散任一集合，那么VC(H)=∞）</p>
<p>​    例如，考虑下面三个点的情况：</p>
<p><img src="/image/20170925/17.png" alt="17"></p>
<p>​    一个在二维空间中的分类器（\(h _{\theta }(x)=1\left \{ \theta _{0}+\theta _{1}x _{1} +\theta _{2}x _{2} \geq 0  \right \}\)）集合H可以打散上面的点吗？答案是可以的。我们看到，对于下面8种情况，我们都可以找到一种分类器可以实现“0训练误差”：</p>
<p><img src="/image/20170925/18.png" alt="18"></p>
<p>​    更多地，可以说明这个假设集H无法打散含有4个点的集合。因此，这个假设集可以打散的集合的大小为3，因此VC(H)=3。</p>
<p>​    注意到，即使有一些3个点的情况不能被打散，但是H的VC维依然为3。例如，如果我们有三个点在一条直线上（左图），然后无法找到一个线性分类器可以分离右图中的三个点：</p>
<p>​    <img src="/image/20170925/19.png" alt="19"></p>
<blockquote>
<p>对于一个包含二维平面分类器的假设集H可以把一个样本集合打散。要注意的是，首先我们有一些固定分布的样本。这些样本的标签是任意的，含有样本的分布情况可能是多种，比如包含三个样本的集合，他们的含有标签分布情况为8种。如果假设集H中的分类器可以把这8种情况都分开，我们就说H可以把含有3个点的集合打散。假设集H的VC维是3。</p>
<p>而并不是说，假设集H要把所有分布情况的样本都分开。比如，若三个样本点呈直线分布，假设集H就无法把某些样本标签的分布分开，如上面左图的情况。但是我们依然可以说假设集H的VC维就是3。因为已经有一种分布的情况，假设集H可以把样本集合H打散了。</p>
<p>对于四个样本点的情况，我们可以发现无论4个样本的标签是怎么样的。假设集H都无法将它们完全分开。</p>
<p>所以我们可以得到结论一个包含二维平面分类器的假设集H的VC维是3。</p>
</blockquote>
<p>​    下面的理论，来自Vapnik，</p>
<p><strong>定理。</strong>给定假设集H，设d=VC(H)。那么要使所有\(h \in H\)有至少1-δ的概率满足，</p>
<p><img src="/image/20170925/20.png" alt="20"></p>
<p>​    则，有至少1-δ的概率，可以得到：</p>
<p><img src="/image/20170925/21.png" alt="21"></p>
<p>​    换句话说，如果一个假设集有有限的VC维，那么当m很大时，一致性假设成立。正如前面所推导的，这可以给出\(\varepsilon(h)\)相对于\(\varepsilon(h ^{\star})\)的边界。我们也可以得到下面的推论：</p>
<p><strong>推论。</strong> \(\left | \varepsilon (h)-\hat{\varepsilon}(h) \right |\leq  \gamma\)对所有\(h \in H\)成立的概率大于1-δ，要满足\(m=O _{\gamma,\delta}(d)\)。</p>
<p>​    换句话说，对于一个假设集H，要取得很好地训练效果所需要的样本个数与VC维有关。这说明，对于大多数的假设类型，VC维与参数的数目也呈现大概的线性关系。最后，我们可以得到，训练集样本的大小与假设集H的参数个数也有线性关系。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/17/0917-ml-05/" rel="next" title="机器学习系列(5)——支持向量机">
                <i class="fa fa-chevron-left"></i> 机器学习系列(5)——支持向量机
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/30/0930-ml-07/" rel="prev" title="机器学习系列(7)——正则化和模型选择">
                机器学习系列(7)——正则化和模型选择 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="lizemin" />
          <p class="site-author-name" itemprop="name">lizemin</p>
           
              <p class="site-description motion-element" itemprop="description">JUST DO IT!</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/li-zemin/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-偏差-方差均衡"><span class="nav-number">1.</span> <span class="nav-text">1. 偏差-方差均衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-准备工作"><span class="nav-number">2.</span> <span class="nav-text">2. 准备工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-有限H的情况"><span class="nav-number">3.</span> <span class="nav-text">3. 有限H的情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-无限大小的H的情况"><span class="nav-number">4.</span> <span class="nav-text">4. 无限大小的H的情况</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lizemin</span>

  
</div>


  <div class="powered-by">
    由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
  </div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">
    主题 &mdash;
    <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
      NexT.Gemini
    </a>
  </div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://lizemin.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/09/25/0925-ml-06/';
          this.page.identifier = '2017/09/25/0925-ml-06/';
          this.page.title = '机器学习系列(6)——学习理论';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://lizemin.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("5lq9RkmkMEG3RVodwAtCuIN9-gzGzoHsz", "3zvKKs0K2UVy7Wz0R1kkjl9R");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
