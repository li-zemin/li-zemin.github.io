<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="算法," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="1. 生成学习算法​    到目前为止，我们主要讨论了对于模型\(p(y|x;\theta)\)的学习算法，表示给定x情况下y的条件分布。在本节中，我们讨论一种不同的学习算法。 ​    假设我们要根据特征预测一只动物是狗还是大象。一种方法是，我们可以根据训练集，得到一个逻辑斯特回归模型或者感知机模型，这样就可以通过一个分类边界预测该动物是狗或者是大象。">
<meta name="keywords" content="算法">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习系列(4)——生成学习算法">
<meta property="og:url" content="http://yoursite.com/2017/09/08/0908-ml-04/index.html">
<meta property="og:site_name" content="Lizemin&#39;s Blog">
<meta property="og:description" content="1. 生成学习算法​    到目前为止，我们主要讨论了对于模型\(p(y|x;\theta)\)的学习算法，表示给定x情况下y的条件分布。在本节中，我们讨论一种不同的学习算法。 ​    假设我们要根据特征预测一只动物是狗还是大象。一种方法是，我们可以根据训练集，得到一个逻辑斯特回归模型或者感知机模型，这样就可以通过一个分类边界预测该动物是狗或者是大象。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/image/20170908/001.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/002.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/003.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/004.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/005.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/006.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/007.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/008.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/009.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/010.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/011.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/012.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/013.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/014.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/015.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/016.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/017.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/018.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/019.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/020.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/021.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/022.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/023.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/024.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/025.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/026.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/027.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/028.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/029.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/030.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/031.png">
<meta property="og:image" content="http://yoursite.com/image/20170908/032.png">
<meta property="og:updated_time" content="2017-10-03T01:43:54.553Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习系列(4)——生成学习算法">
<meta name="twitter:description" content="1. 生成学习算法​    到目前为止，我们主要讨论了对于模型\(p(y|x;\theta)\)的学习算法，表示给定x情况下y的条件分布。在本节中，我们讨论一种不同的学习算法。 ​    假设我们要根据特征预测一只动物是狗还是大象。一种方法是，我们可以根据训练集，得到一个逻辑斯特回归模型或者感知机模型，这样就可以通过一个分类边界预测该动物是狗或者是大象。">
<meta name="twitter:image" content="http://yoursite.com/image/20170908/001.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/08/0908-ml-04/"/>





  <title>机器学习系列(4)——生成学习算法 | Lizemin's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lizemin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/08/0908-ml-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lizemin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lizemin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习系列(4)——生成学习算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-08T11:36:40+08:00">
                2017-09-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          
             <span id="/2017/09/08/0908-ml-04/" class="leancloud_visitors" data-flag-title="机器学习系列(4)——生成学习算法">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-生成学习算法"><a href="#1-生成学习算法" class="headerlink" title="1. 生成学习算法"></a>1. 生成学习算法</h2><p>​    到目前为止，我们主要讨论了对于模型\(p(y|x;\theta)\)的学习算法，表示给定x情况下y的条件分布。在本节中，我们讨论一种不同的学习算法。</p>
<p>​    假设我们要根据特征预测一只动物是狗还是大象。一种方法是，我们可以根据训练集，得到一个逻辑斯特回归模型或者感知机模型，这样就可以通过一个分类边界预测该动物是狗或者是大象。<br><a id="more"></a><br>​    也有另一种不同的方法。首先，通过观察大象，我们可以建立大象的模型。然后，观察狗，我们建立狗的模型。最后，为了对一个新的动物进行分类，我们可以将这个动物与两个模型匹配比较。去对比发现这个动物更像大象还是更像狗。以此得到最后的预测结果。</p>
<p>​    直接学习得到\(p(y|x)\)结果的算法（例如Logistic回归）或者试图学习得到从输入空间X到样本标签{0，1}的假设的算法被称为<strong>判别学习算法(discriminative learning algorithms)</strong>。在本节中，我们将讨论对\(p(x|y)\)和\(p(y)\)进行建模。这种算法被称为<strong>生成学习算法(generative learning algorithms)</strong>。例如，如果y表示一个样本是狗(0)或者是大象(1)，那么\(p(x|y)=0\)表示狗的特征的模型分布，而\(p(x|y)=1\)表示大象的特征的模型分布。</p>
<p>​    之后对\(p(y)\)(被称为<strong>先验概率</strong>)进行建模，有一种被称为<strong>贝叶斯法则</strong>可以得到给定x下的y的分类：<br>    <img src="/image/20170908/001.png" alt="001"><br>​    该式中，分母\(p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)\)。事实上，如果我们要通过计算p(y|x)进行预测的话，分母的值并不需要计算出来，因此：<br>​    <img src="/image/20170908/002.png" alt="002"></p>
<h2 id="2-高斯判别分析"><a href="#2-高斯判别分析" class="headerlink" title="2. 高斯判别分析"></a>2. 高斯判别分析</h2><p>​    我们即将接触的第一个生成学习算法是<strong>高斯判别分析(GDA, Gaussian discriminant analysis)</strong>。在这个模型中，我们假设\(p(x|y)\)是多元正态分布的。我们先了解多元正态分布的一些特性。</p>
<h3 id="2-1-多元正态分布"><a href="#2-1-多元正态分布" class="headerlink" title="2.1 多元正态分布"></a>2.1 多元正态分布</h3><p>​    在n维空间中的多元正态分布，也被称为多元高斯分布，主要由<strong>均值向量</strong>\(\mu\in\mathbb{R}^{n}\)和<strong>协方差矩阵</strong>\(\Sigma \in \mathbb{R}^{n\times n}\)定义，同时 Σ 为对称的正定矩阵。可以写为“\(N(\mu,\Sigma)\)”，概率密度为：<br><img src="/image/20170908/003.png" alt="003"><br>​    上式中，“\(|\Sigma|\)”表示矩阵Σ的行列式的值。</p>
<p>​    对于一个满足\(N(\mu,\Sigma)\)分布的变量X，平均值为μ：<br>​    <img src="/image/20170908/004.png" alt="004"><br>​    随机向量Z的协方差定义为\(Cov(Z)=E[(Z-E[Z])(Z-E[Z]) ^{T}]\)。也可以被定义为\(Cov(Z)=E[ZZ^{T}]-(E[Z])(E[Z])^{T}\)。如果\(X\sim (\mu,\Sigma)\)，则<br>$$<br>Cov(X)=\Sigma<br>$$</p>
<p>​    对随机变量Z的协方差定义做一个转换解释：<br>$$<br>Cov(Z)=E[(Z-E(X)) ^{2}]\\<br>=E[X^{2}-2E[X]X+E[X] ^{2}]\ <br>=E[X^{2}]-2E[X]E[X]+E[X] ^{2}\\<br>=E[X^{2}]-E[X] ^{2}<br>$$</p>
<p>​    对于如下所示的高斯密度分布：<br>​    <img src="/image/20170908/005.png" alt="005"><br>​    最左边的图展示了一个均值为0(\(2\times1\)的0向量)，协方差矩阵为单位矩阵(\(2\times2\)的单位矩阵)的高斯分布，这样的高斯分布也被成为<strong>标准分布</strong>。中间的图展示了一种协方差为0.6I，均值为0的高斯分布，右边的图展示了协方差矩阵为2I的高斯分布。我们可以看到：<strong>协方差矩阵越大，高斯分布的图像越扁平，协方差矩阵越小，高斯分布的图像越瘦长</strong>。</p>
<p>​    接下来，我们看这样一些例子。<br>​    <img src="/image/20170908/006.png" alt="006"><br>​    上图中的高斯分布的均值都为0，协方差矩阵分别为：<br>​    <img src="/image/20170908/007.png" alt="007"><br>​    最左边的图为标准高斯分布，随着协方差矩阵逆对角线方向的值的增加，密度分布逐渐在45°的方向上发生“积压”。通过三张图形的等高线，可以清晰看到这一趋势。<br>​    <img src="/image/20170908/008.png" alt="008"><br>​    下面给出另外一些当协方差矩阵Σ变化时的密度分布等高线图：<br>​    <img src="/image/20170908/009.png" alt="009"><br>​    上图中图形的协方差矩阵分别为：<br>​    <img src="/image/20170908/010.png" alt="010"><br>​    从最左边的图和中间的图我们可以看到，随着减少协方差矩阵对角线的值，在相反的方向上密度分布再次被“挤压”。最后，参数变化时，等高线常为椭圆形。<br>​    最高固定协方差矩阵Σ为单位矩阵，变化μ时，可以得到下面的密度分布图：<br>​    <img src="/image/20170908/011.png" alt="011"><br>​    对应的μ为：<br>​    <img src="/image/20170908/012.png" alt="012"></p>
<h3 id="2-2-高斯判别分析模型"><a href="#2-2-高斯判别分析模型" class="headerlink" title="2.2 高斯判别分析模型"></a>2.2 高斯判别分析模型</h3><p>​    对于输入特征 <em>x</em> 为连续变量的分类问题，我们可以使用高斯判别分析（GDA）模型，就是使用一个多维正太分布对<em>p(x|y)</em> 进行建模，对应的模型为：<br>​    <img src="/image/20170908/013.png" alt="013"><br>​    即，<br>​    <img src="/image/20170908/014.png" alt="014"><br>​    模型的参数为Φ，Σ，\(\mu _{0}\)以及\(\mu _{1}\)。（注意，有两个不同的均值向量\(\mu _{0}\)和\(\mu _{1}\)，模型通常只有一个协方差矩阵Σ）。这些参数的极大似然对数估计为，<br>​    <img src="/image/20170908/015.png" alt="015"></p>
<blockquote>
<p>对于该式极大化推导，可以<strong>参考博文：</strong><br>​    <strong> 1. <a href="http://blog.csdn.net/zhulf0804/article/details/52474339" target="_blank" rel="external">朴素贝叶斯算法参数的最大似然估计</a></strong><br>​    <strong> 2. <a href="http://blog.csdn.net/sz464759898/article/details/44342923" target="_blank" rel="external">高斯判别分析和朴素贝叶斯</a></strong></p>
</blockquote>
<p>​    极大化似然函数后，我们得到了对参数的极大似然评估，<br>​    <img src="/image/20170908/016.png" alt="016"></p>
<blockquote>
<p>这些结果是通过似然函数求导得出的，同样我们可以根据对实际问题的分析得到这样的式子。例如，<br>Φ表示样本为1的概率，即为标签为1的样本个数除以总样本数；<br>\(\mu _{0}, \mu _{1}\)分别表示标签为0，1的样本特征x的平均值；<br>Σ表示样本的协方差矩阵。</p>
</blockquote>
<p>​    该算法完成的工作可以从下图中体现：<br>​    <img src="/image/20170908/017.png" alt="017"></p>
<p>​    图中的×和O表示两种标签的样本分布，在每种样本分布区域中都有一个表示高斯分布的椭圆。注意，两个高斯分布有相同形状和方向的椭圆，因为他们有相同的协方差矩阵Σ，但是有不同的均值\(\mu _{0}, \mu _{1}\)。图中的直线表示了在p(y=1|x)=0.5时的分类边界。在边界一边，我们预测输出更可能为y=1，在另一边，输出更可能为y=0。</p>
<h3 id="2-3-GDA与-Logistic分布"><a href="#2-3-GDA与-Logistic分布" class="headerlink" title="2.3 GDA与 Logistic分布"></a>2.3 GDA与 Logistic分布</h3><p>​    GDA模型与Logistic分布之间的联系很有趣。如果我们将\(p(y=1|x;\phi,\mu _{0},\mu_{1},\Sigma)\)看成x的函数，可以表示为（经证明后成立，特征分布为泊松分布时也成立），<br>​    <img src="/image/20170908/018.png" alt="018"></p>
<p>​     p(y=1|x)可以通过下式求出：<br>$$<br>p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)}\\<br>p(x)=p(x|y=0)p(y=0)+p(x|y=1)p(y=1)<br>$$<br>​    这里，θ可以看成Φ，Σ，μ0，μ1的函数。我们就得到了逻辑斯特回归，一种判别算法，建立了p(y=1|x)的模型。</p>
<p>​    在相同的训练集上进行训练，<strong>GDA和Losigtic会得到不同的决策边界，哪一个更优呢？</strong></p>
<p>​    刚刚证明了，<strong>如果p(x|y)是为多维高斯分布（同一个Σ），那么必然地p(y|x)符合逻辑斯特回归函数。然而，反过来是不成立的。p(y|x)为逻辑斯特函数时，并不能推出p(x|y)为高斯分布</strong>。这说明，GDA模型比逻辑斯特回归模型对数据有更强的假设要求。在模型假设时正确的情况下，GDA可以找到更好地匹配数据地模型。特别地，当p(x|y)的确满足高斯分布时，GDA时<strong>渐进有效</strong>的。也可以这么说，这意味着在较大数据集的限制下，没有比GDA严格更好的算法。特别地，在这种情况下，GDA是比逻辑斯特更好的算法。</p>
<p>​    相比之下，在更弱的假设下，逻辑斯特回归鲁棒性更好，对错误的模型假设敏感度更低。有不同的假设情况，使p(y|x)符合逻辑斯特函数。例如，当x|y 满足泊松分布时，p(y|x)也是一个逻辑斯特函数。逻辑斯特回归在泊松数据集上也表现地很好。但是，如果我们在这样地数据上应用GDA模型，将高斯分布假设置于一个非高斯分布地数据集上，结果是不可预估得，也许GDA可以取得很好地效果。</p>
<p>​    总而言之：<strong>GDA需要更强的假设，当模型假设正确或至少大体上正确时，训练效率更高（需要较少的数据就可以取得很好地训练效果）。逻辑斯特回归的假设更弱，鲁棒性更好。特别得，当数据是非高斯的并且数据集较大，逻辑斯特回归比GDA的效果更好。正因如此，事实上，逻辑斯特回归比高斯回归更常用</strong>。</p>
<h2 id="3-朴素贝叶斯"><a href="#3-朴素贝叶斯" class="headerlink" title="3. 朴素贝叶斯"></a>3. 朴素贝叶斯</h2><p>​    在高斯判别分析（GDA）中，特征向量x是连续的实数向量。我们接下来讨论一种对于x是离散值的学习算法。</p>
<p>​    垃圾邮件分类是<strong>文本分类</strong>中的典型问题。我们将一封邮件用长度为字典中的词数目的向量来表示。具体讲，如果一封邮件中包含字典中的第i个词，我们就使\(x _{i}=1\)；否则，\(x _{i}=0\)。例如，如下向量：<br>​    <img src="/image/20170908/019.png" alt="019"></p>
<p>​    表示一封邮件中，包含单词“a”，“buy”，但是不包含“aardvark”，”aardwolf“。这样就构造出了一个<strong>词汇表</strong>。</p>
<p>​    在选择好特征向量之后，我们要建立判别模型。所以，我们要对p(x|y)进行建模。但是，如果我们有一个包含50000个单词的词汇表，即\(x\in \left \{ 0,1 \right \} ^{50000}\)（x是一个50000维的向量，只有元素0和1）。并且，如果我们明确使用多维分布对\(2 ^{50000}\)个输出进行建模，那么我们就会得到一个\(2 ^{50000}-1\)维的参数向量。参数数目确实十分巨大。</p>
<p>​    为了对p(x|y)进行建模，我们要做一个很强的假设。我们假设\(x _{i}\)是在y条件下的独立分布。这个假设被称为<strong>朴素贝叶斯假设(Naive Bayes assumption)</strong>，由此得出的算法成为<strong>朴素贝叶斯分类器(Naive Bayes classifier)</strong>。例如，如果y=1表示垃圾邮件；”buy“为单词2087以及”price“为单词39831；然后我们假设，如果已知y=1，那么\(x _{2087}\)的概率对\(x _{39831}\)的概率没有影响。正式来讲，可以写为\(p(x _{2087}|y)=p(x _{2087}|y,x _{39831})\)。（注意，这并不是意味着\(x _{2087}\)与\(x _{39831}\)是独立的，可以被写为\(p(x _{2087}|y)=p(x _{2087}|x _{39831}))；而我们只是假设\(x _{2087}\)与\(x _{39831}\)在y给定条件下是条件独立的）</p>
<p>​    现在，我们有：<br>​    <img src="/image/20170908/020.png" alt="020"></p>
<p>​    第一个等式由概率的基本特性得出，第二个等式来源于朴素贝叶斯假设。即使朴素贝叶斯假设是一个很强的设定，但是算法最后的效果依然很好。</p>
<p>​    我们的模型可以参数化为\(\phi _{i|y=1}=p(x _{i}=1|y=1),\phi _{i|y=0}=p(x _{i}=1|y=0),\phi _{y}=p(y=1)\)。通常，给定训练集\(\left \{ (x ^{(i)}, y ^{(i)});i=1,…,m\right\}\)，可以写出这些参数的联合似然估计：<br>​    <img src="/image/20170908/021.png" alt="021"></p>
<p>​    最大化似然估计，可以得到参数为：<br>​    <img src="/image/20170908/022.png" alt="022"></p>
<p>​    在上式中，\(\wedge \)表示”与“。这些参数由非常自然的假设，例如\(\phi _{j|y=1}\)表示在垃圾邮件中单词j出现的概率。</p>
<p>​    找到每个合适的参数后，对于一个新样本的预测，我们可以计算，    <img src="/image/20170908/023.png" alt="023"></p>
<p>​    然后判断该样本属于哪个类别的概率更大。</p>
<p>​    最后，我们注意到，当我们推导对特征x只有两种取值的贝叶斯算法问题时，也会考虑\(x _{i}\)可以由k个取值。这里，我们对多值情况下的p(x _i|y)进行简单建模。确实，即使原始输入参数时连续值，我们也可以将其离散化。例如，对于房屋面积是连续值这一情况，我们可以将其离散化为：<br><img src="/image/20170908/024.png" alt="024"></p>
<p>​    然后使用多项式分布对p(x|y)进行建模（正如前面所推导的）。连续分布的输入用多维正太分布并不一定可以很好建模，而将特征离散化并且使用朴素贝叶斯（而不是GDA）通常效果更好。</p>
<h3 id="3-1-Laplace平滑"><a href="#3-1-Laplace平滑" class="headerlink" title="3.1 Laplace平滑"></a>3.1 Laplace平滑</h3><p>​    在垃圾邮件统计问题中，可能存在这样的情况：可能某一天你发了一封包含一个专有名词“NIPS”(机器学习领域顶会)的邮件，这个词在原先的邮件中没有出现过。假设“nips”是字典中的第35000个单词，在朴素贝叶斯垃圾邮件过滤器中对参数\(\phi _{35000|y}\)进行极大似然估计会是，<br>​    <img src="/image/20170908/025.png" alt="025"></p>
<p>​    因为，这个新单词在之前的垃圾邮件以及非垃圾邮件中都未出现过，所以认为它在每种类型的概率都为0。因此，决包含“nips”的邮件属于哪种类别时，会得到，<br>​    <img src="/image/20170908/026.png" alt="026"></p>
<p>​    这是因为该式中包含\(p(x _{35000}|y)=0\)。因此， 我们无法得到正确的结论。</p>
<p>​    我们可以更广泛的描述这一问题，就是我们很难去评估一个从未在训练集中出现的样本的概率。有一个多维随机变量取值为{1, ….. ,k}。我们可以通过\(\phi _{i}=p(z=i)\)评估多维分布。假设有m个观察样本，则z第j个取值的极大似然估计为，<br>​    <img src="/image/20170908/027.png" alt="027"></p>
<p>​    正如我们前面所提到的，这个估计可能为0。为了避免这种情况的产生，我们使用<strong>拉普拉斯平滑（Laplace smoothing）</strong>，可以代替最初的估计，<br>​    <img src="/image/20170908/028.png" alt="028"><br>​    最后，z的每个取值的概率和仍然为1。在朴素贝叶斯分类器中，我们应用拉普拉斯平滑，可以得到新的参数估计，<br>​    <img src="/image/20170908/029.png" alt="029"></p>
<p>​    事实上，我们无需考虑\(\phi _{y}\)是否为0，因为计算该样本是垃圾邮件还是非垃圾邮件时，它们有共同的分母，因此，我们可以不计算这个分母。</p>
<h3 id="3-2-文本分类中的事件模型"><a href="#3-2-文本分类中的事件模型" class="headerlink" title="3.2 文本分类中的事件模型"></a>3.2 文本分类中的事件模型</h3><p>​    在即将结束对生成学习算法讨论的时候，我们关注一个文本分类的问题。在许多分类问题中，朴素贝叶斯可以取得很好地效果，对于文本分类问题，有一种相关的模型可以表现得更好。</p>
<p>​    在文本分类这种特殊环境下，朴素贝叶斯可以提供一种<strong>多元伯努利事件模型(multi-variate Bernoulli event model)</strong>。在这个模型中，我们假设有限邮件是随机确定是否为垃圾邮件，然后会发给你一个消息。然后，将客户发送的邮件在字典中进行扫描，然后决定是否在这个邮件中包含单词i，并且得到概率\(p(x_{i}=1|y)=\phi _{i|y}\)。因此，这个消息给出的概率为\(p(y)\prod _{i=1}^{n}p(x _{i}|y)\)。</p>
<p>​    这里介绍一种不同的模型，称为<strong>多元事件模型（multinomial event model）</strong>。为了描述这个模型， 我们使用一种对已有邮件特征不同的描述标记。\(x _{i}\)表示第i个单词在邮件中的标记。因此，\(x _{i}\)为整型数取值为\(\left \{ 1,…,\left | V \right | \right \}\)，\(\left | V \right |\)表示词汇表的大小。一封包含n个单词的邮件可以通过一个长度为n的向量\((x _{1},x _{2},…,x _{n})\)表示，n会随着邮件的长短变化。</p>
<p>​    在多元事件模型中，我们假设随机确定是否为垃圾邮件。然后，发件方通过从单词的多维随机分布\(p(x _{1}|y)\)中得到第一个单词\(x _{1}\)。接着，\(x _{2}\)独立与\(x _{1}\)，从多维随机分布中得出，\(x _{3}\)，\(x _{4}\)类似得出，直到n个单词。因此，这个消息的概率为，\(p(y)\prod _{i=1}^{n}p(x _{i}|y)\)。注意到，这个公式看起来类似与我们在多元伯努利事件模型中得到的式子，但是，它们的含义是不一样的。尤其是，\(x _{x}|y\)现在是一个多维分布，而不是伯努利分布。</p>
<p>​    新模型的参数\(\phi _{y}=p(y)\)和原先一致，\(\phi _{i|y=1}=p(x _{j}=i|y=1)\)以及\(\phi _{i|y=0}=p(x _{j}=i|y=0)\)。注意，我们已经假设\(p(x _{j}=i|y)\)对于所有的j都是相同的。</p>
<p>​    如果我们给定一个训练集\(\left \{ (x ^{(i)},y ^{(i)});i=1,…,m \right \}\)，\(x ^{(i)}=\left ( x _{1} ^{(i)}, x _{2} ^{(i)},…, x _{n _{i}} ^{(i)} \right )\)。则参数的似然估计为，<br>​    <img src="/image/20170908/030.png" alt="030"></p>
<p>​    最大化似然估计可以得到参数，<br>​    <img src="/image/20170908/031.png" alt="031"></p>
<p>​    对比与原先模型推出的参数，可以看到，多元伯努利事件模型不考虑没个词出现的次数。新模型考虑了每个单词在文本中出现的次数。因此某个单词在有一类样本中出现的概率为：该类样本中该词出现的总次数/该类样本中的总次数。</p>
<blockquote>
<p>个人理解为，在多元伯努利事件模型中，我们关注于在一封邮件中，每个单词的分布，满足伯努利分布。</p>
<p>在多维事件模型中，我们关注于邮件中的每个位置，每个位置都满足多项分布，由此可以考虑到某个单词在出现的概率。</p>
<p>这两者的差异，主要来源于最初的假设。可以看到，在前面的模型中，参数为\(\phi _{i|y=1}=p(x _{i}=1|y=1),\phi _{i|y=0}=p(x _{i}=1|y=0),\phi _{y}=p(y=1)\)；在新的模型中，参数为，\(\phi _{i|y=1}=p(x _{j}=i|y=1)\)以及\(\phi _{i|y=0}=p(x _{j}=i|y=0)\)，\(\phi _{y}=p(y)\)。</p>
</blockquote>
<p>​    如果我们应用拉普拉斯平滑，可以得到，<br>​    <img src="/image/20170908/032.png" alt="032"></p>
<p>​    然而如果没有必要寻找最好的分类器，朴素贝叶斯就是很好地选择。它经常被称为“第一次尝试”，这说明了实现的容易程度。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/05/0904-ml-03/" rel="next" title="机器学习系列(3)——广义线性模型">
                <i class="fa fa-chevron-left"></i> 机器学习系列(3)——广义线性模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/17/0917-ml-05/" rel="prev" title="机器学习系列(5)——支持向量机">
                机器学习系列(5)——支持向量机 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="lizemin" />
          <p class="site-author-name" itemprop="name">lizemin</p>
           
              <p class="site-description motion-element" itemprop="description">JUST DO IT!</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/li-zemin/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-生成学习算法"><span class="nav-number">1.</span> <span class="nav-text">1. 生成学习算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-高斯判别分析"><span class="nav-number">2.</span> <span class="nav-text">2. 高斯判别分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-多元正态分布"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 多元正态分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-高斯判别分析模型"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 高斯判别分析模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-GDA与-Logistic分布"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 GDA与 Logistic分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-朴素贝叶斯"><span class="nav-number">3.</span> <span class="nav-text">3. 朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Laplace平滑"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Laplace平滑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-文本分类中的事件模型"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 文本分类中的事件模型</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lizemin</span>

  
</div>


  <div class="powered-by">
    由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
  </div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">
    主题 &mdash;
    <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
      NexT.Gemini
    </a>
  </div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://lizemin.disqus.com/count.js" async></script>
    

    

  




	





  










  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("5lq9RkmkMEG3RVodwAtCuIN9-gzGzoHsz", "3zvKKs0K2UVy7Wz0R1kkjl9R");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
